# graphtune/rag/traffic_docs.py
"""
traffic_docs
============

ì‹œê³„ì—´ + ê·¸ë˜í”„ ê¸°ë°˜ êµí†µ ë°ì´í„°ë¥¼
RAGì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” 'í…ìŠ¤íŠ¸ ë¬¸ì„œ(TrafficDoc)' í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ìœ í‹¸ë¦¬í‹°.

í˜„ì¬ ë²„ì „:
- train_loader ê¸°ì¤€ìœ¼ë¡œ ê° ë…¸ë“œ(node)ì— ëŒ€í•œ ì „ì²´ íˆìŠ¤í† ë¦¬ í†µê³„ë¥¼ ê³„ì‚°
- ë…¸ë“œë³„ë¡œ 1ê°œì”© "ì „ì²´ ê¸°ê°„ ìš”ì•½" ë¬¸ì„œë¥¼ ìƒì„±

í–¥í›„ í™•ì¥ ì•„ì´ë””ì–´:
- ì‹œê°„ëŒ€(ì¶œê·¼/í‡´ê·¼), ìš”ì¼ë³„, ë‚ ì”¨ ì¡°ê±´ë³„ë¡œ ë” ì„¸ë¶„í™”ëœ ë¬¸ì„œ ìƒì„±
- íŠ¹ì • ê¸°ê°„(ìµœê·¼ 7ì¼, ìµœê·¼ 30ì¼ ë“±)ë§Œ ë”°ë¡œ ìš”ì•½
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Any, List, Optional, Tuple

import numpy as np
import torch


@dataclass
class TrafficDoc:
    """
    RAG ì¸ë±ìŠ¤ì— ë“¤ì–´ê°ˆ ê¸°ë³¸ ë‹¨ìœ„ ë¬¸ì„œ.

    doc_id      : ê³ ìœ  ì‹ë³„ì (ì˜ˆ: "songdo_node_12_full")
    city        : ë„ì‹œ ì´ë¦„ (ì˜ˆ: "songdo", "pems-bay")
    node_index  : ë…¸ë“œ ì¸ë±ìŠ¤ (0-based)
    node_name   : ì„¼ì„œ ID ë“± ì‚¬ëŒ ì¹œí™”ì ì¸ ì´ë¦„ (ì—†ìœ¼ë©´ None)
    time_span   : ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ ì‹œê°„ ë²”ìœ„ ì„¤ëª… (ì˜ˆ: "full_history")
    summary_text: LLM ì»¨í…ìŠ¤íŠ¸ë¡œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìì—°ì–´ ìš”ì•½
    stats       : ìˆ˜ì¹˜ ìš”ì•½ (mean, std, max, count ë“±)
    """
    doc_id: str
    city: str
    node_index: int
    node_name: Optional[str]
    time_span: str
    summary_text: str
    stats: Dict[str, float]


def _compute_node_stats_from_loader(
    loader: torch.utils.data.DataLoader,
    scaler: Optional[Any] = None,
    max_batches: Optional[int] = None,
) -> Dict[str, np.ndarray]:
    """
    DataLoaderì—ì„œ (batch, T, N) í˜•íƒœì˜ ì‹œê³„ì—´ì„ ì½ì–´
    ë…¸ë“œë³„ íˆìŠ¤í† ë¦¬ í†µê³„(mean, std, max, count)ë¥¼ ê³„ì‚°í•œë‹¤.

    - X(ì…ë ¥ ì‹œê³„ì—´)ë§Œ ì‚¬ìš© (yëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
    - scalerê°€ ì£¼ì–´ì¡Œìœ¼ë©´ inverse_transformì„ ì ìš©í•´ ì›ë˜ ìŠ¤ì¼€ì¼ì—ì„œ í†µê³„ë¥¼ ê³„ì‚°
    - ë°ì´í„° ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ì§€ ì•Šê³ , ë°°ì¹˜ ë‹¨ìœ„ë¡œ ëˆ„ì  ê³„ì‚°
    """
    node_sum = None
    node_sumsq = None
    node_max = None
    total_count = 0

    for batch_idx, batch in enumerate(loader):
        if max_batches is not None and batch_idx >= max_batches:
            break

        # batchëŠ” (x, y, mask) êµ¬ì¡°ë¼ê³  ê°€ì •
        if isinstance(batch, (list, tuple)) and len(batch) >= 1:
            x = batch[0]
        else:
            x = batch

        # ğŸ”´ ì—¬ê¸°ì„œë¶€í„°ëŠ” GPU í…ì„œë“  ë­ë“  ì¼ë‹¨ CPU numpyë¡œ ë³€í™˜
        x_np = x.detach().cpu().numpy()

        # ğŸ”§ scalerê°€ ìˆìœ¼ë©´ ì›ë˜ ë‹¨ìœ„ë¡œ inverse_transform
        if scaler is not None:
            try:
                # loaders.pyì—ì„œ normí•  ë•Œì™€ ë™ì¼í•˜ê²Œ ë§ˆì§€ë§‰ ì¶•ì„ nodeë¡œ ë³´ê³  flatten
                shape = x_np.shape            # (B, T, N)
                flat = x_np.reshape(-1, shape[-1])   # (B*T, N)
                flat_inv = scaler.inverse_transform(flat)  # (B*T, N)
                x_np = flat_inv.reshape(shape)      # ë‹¤ì‹œ (B, T, N)
            except Exception as e:
                # í•œ ë²ˆë§Œ ê²½ê³  ì°ê³ , ì´í›„ì—ëŠ” ê·¸ëƒ¥ ì •ê·œí™” ìŠ¤ì¼€ì¼ë¡œ ì§„í–‰
                if batch_idx == 0:
                    print(f"[traffic_docs] warning: scaler.inverse_transform ì‹¤íŒ¨, ì •ê·œí™” ìŠ¤ì¼€ì¼ì—ì„œ í†µê³„ ê³„ì‚° ({e})")

        # ğŸ“Š í†µê³„ ê³„ì‚°ìš©ìœ¼ë¡œ ë§ˆì§€ë§‰ ì¶•(N)ì„ ë…¸ë“œë¡œ ë³´ê³  ë‚˜ë¨¸ì§€ë¥¼ ëª¨ë‘ í¼ì¹˜ê¸°
        if x_np.ndim < 2:
            continue

        if x_np.ndim == 2:
            flat_stats = x_np   # (B, N)
        else:
            flat_stats = x_np.reshape(-1, x_np.shape[-1])  # (..., N)

        if node_sum is None:
            N = flat_stats.shape[1]
            node_sum = flat_stats.sum(axis=0)
            node_sumsq = (flat_stats ** 2).sum(axis=0)
            node_max = flat_stats.max(axis=0)
        else:
            node_sum += flat_stats.sum(axis=0)
            node_sumsq += (flat_stats ** 2).sum(axis=0)
            node_max = np.maximum(node_max, flat_stats.max(axis=0))

        total_count += flat_stats.shape[0]

    if node_sum is None or total_count == 0:
        raise ValueError("[traffic_docs] loaderì—ì„œ ìœ íš¨í•œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    mean = node_sum / total_count
    var = node_sumsq / total_count - mean ** 2
    var = np.maximum(var, 1e-6)
    std = np.sqrt(var)

    stats = {
        "mean": mean,
        "std": std,
        "max": node_max,
        "count": total_count,
    }
    return stats


def build_node_level_docs_from_bundle(
    bundle: Dict[str, Any],
    city: str,
    max_batches: Optional[int] = None,
    time_span_label: str = "full_history",
) -> List[TrafficDoc]:
    """
    GraphTuneì˜ dataset bundleë¡œë¶€í„°
    'ë…¸ë“œë³„ ì „ì²´ íˆìŠ¤í† ë¦¬ ìš”ì•½ ë¬¸ì„œ' ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.

    Parameters
    ----------
    bundle: dict
        prepare_dataset(...)ê°€ ë°˜í™˜í•˜ëŠ” dict.
        ë‹¤ìŒ í‚¤ë“¤ì„ ì‚¬ìš©í•œë‹¤:
        - "train_loader": DataLoader
        - "scaler": StandardScaler (ì„ íƒ)
        - "num_nodes": int (ì„ íƒ, ì—†ìœ¼ë©´ adjacencyë¡œë¶€í„° ì¶”ë¡ )
        - "A": adjacency matrix (ì„ íƒ)
        - "sensor_ids": ì„¼ì„œ ID ë¦¬ìŠ¤íŠ¸ (ì„ íƒ)

    city: str
        ë„ì‹œ ì´ë¦„(label). ì˜ˆ: "metr-la", "pems-bay", "songdo"

    max_batches: int, optional
        train_loaderì—ì„œ í†µê³„ë¥¼ ê³„ì‚°í•  ë•Œ ì‚¬ìš©í•  ìµœëŒ€ ë°°ì¹˜ ìˆ˜.
        Noneì´ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©.

    time_span_label: str
        ì´ ìš”ì•½ì´ ì»¤ë²„í•˜ëŠ” ì‹œê°„ ë²”ìœ„ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ í‘œí˜„í•œ ë¼ë²¨.
        ì˜ˆ: "full_history", "recent_30_days" ë“±.

    Returns
    -------
    docs: List[TrafficDoc]
        ë…¸ë“œë³„ë¡œ 1ê°œì”© ìƒì„±ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸.
    """
    train_loader = bundle["train_loader"]
    scaler = bundle.get("scaler", None)
    num_nodes = bundle.get("num_nodes", None)
    A = bundle.get("A", None)
    sensor_ids = bundle.get("sensor_ids", None)

    # ë…¸ë“œ ìˆ˜ ì¶”ë¡ 
    if num_nodes is None:
        if A is not None:
            num_nodes = A.shape[0]
        else:
            # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: train_loaderì—ì„œ í•œ ë°°ì¹˜ êº¼ë‚´ì„œ ì¶”ë¡ 
            first_batch = next(iter(train_loader))
            x = first_batch[0] if isinstance(first_batch, (list, tuple)) else first_batch
            num_nodes = x.shape[-1]
            print(f"[traffic_docs] num_nodesë¥¼ DataLoaderì—ì„œ ì¶”ë¡ : {num_nodes}")

    # ë…¸ë“œë³„ í†µê³„ ê³„ì‚°
    stats = _compute_node_stats_from_loader(train_loader, scaler=scaler, max_batches=max_batches)
    mean = stats["mean"]
    std = stats["std"]
    vmax = stats["max"]

    # í‰ê·  ê¸°ì¤€ìœ¼ë¡œ ë…¸ë“œ í˜¼ì¡ë„ ìˆœìœ„ ê³„ì‚° (ë‚´ë¦¼ì°¨ìˆœ)
    order = np.argsort(-mean)  # í° ê°’ì´ ë¨¼ì €
    node_rank = np.empty_like(order)
    node_rank[order] = np.arange(len(order))
    num_nodes_float = float(num_nodes)

    docs: List[TrafficDoc] = []

    for node_idx in range(num_nodes):
        node_mean = float(mean[node_idx])
        node_std = float(std[node_idx])
        node_max = float(vmax[node_idx])
        rank = int(node_rank[node_idx])
        # ìƒìœ„ ëª‡ % í˜¼ì¡ ë…¸ë“œì¸ì§€
        congestion_percentile = 100.0 * (num_nodes_float - rank) / num_nodes_float

        node_name: Optional[str] = None
        if sensor_ids is not None and node_idx < len(sensor_ids):
            node_name = str(sensor_ids[node_idx])

        node_label = node_name if node_name is not None else f"node_{node_idx}"

        # í•œêµ­ì–´ ê¸°ì¤€ì˜ ê°„ë‹¨í•œ ìš”ì•½ í…ìŠ¤íŠ¸ (ë‚˜ì¤‘ì— í…œí”Œë¦¿/í”„ë¡¬í”„íŠ¸ë¡œ íŠœë‹ ê°€ëŠ¥)
        summary_lines = [
            f"{city} êµí†µ ì„¼ì„œ {node_label}ì˜ ì „ì²´ ê´€ì¸¡ ê¸°ê°„ ìš”ì•½ì…ë‹ˆë‹¤.",
            f"- í‰ê·  êµí†µëŸ‰: ì•½ {node_mean:.1f} ë‹¨ìœ„",
            f"- ë³€ë™ì„±(í‘œì¤€í¸ì°¨): ì•½ {node_std:.1f}",
            f"- ê´€ì¸¡ëœ ìµœëŒ€ ê°’: {node_max:.1f}",
            f"- ë‹¤ë¥¸ ë…¸ë“œì™€ ë¹„êµí–ˆì„ ë•Œ í˜¼ì¡ë„ ìƒìœ„ ì•½ {congestion_percentile:.1f}% ìˆ˜ì¤€ì— í•´ë‹¹í•©ë‹ˆë‹¤.",
        ]
        summary_text = "\n".join(summary_lines)

        doc_id = f"{city}_node_{node_idx}_{time_span_label}"

        doc = TrafficDoc(
            doc_id=doc_id,
            city=city,
            node_index=node_idx,
            node_name=node_name,
            time_span=time_span_label,
            summary_text=summary_text,
            stats={
                "mean": node_mean,
                "std": node_std,
                "max": node_max,
                "rank": float(rank),
                "congestion_percentile": congestion_percentile,
                "count": float(stats["count"]),
            },
        )
        docs.append(doc)

    return docs
